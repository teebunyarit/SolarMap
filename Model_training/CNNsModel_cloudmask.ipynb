{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Add, AveragePooling2D, Concatenate\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import concatenate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b837c",
   "metadata": {},
   "source": [
    "# Load dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e170ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "Directory = '/Users/khunnoot/Desktop/year4/senior_project/jupyter_notebook'\n",
    "CNNsDataframe = 'Image_dataset_1layer.csv'\n",
    "AtributeDataframe = 'Attribute_dataset_1layer.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7b5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(Directory, CNNsDataframe),parse_dates=True,index_col='datetime')\n",
    "ATT = pd.read_csv(os.path.join(Directory, AtributeDataframe),parse_dates=True,index_col='datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def81cec",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4596e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define split data by using clear-sky index condition\n",
    "\n",
    "def train_test_splits(df, test_size=0.2, random_state=42, split_by_k = True):\n",
    "        df['date'] = df.index.date\n",
    "        INDEX = df[['site','k']].copy()\n",
    "        INDEX['date']= INDEX.index.date\n",
    "        INDEX = INDEX.groupby(by=[df.index.date,'site']).mean()\n",
    "        INDEX.reset_index(level=1, inplace=True)\n",
    "        \n",
    "        if split_by_k :\n",
    "            klow = INDEX[INDEX.k<=0.3]\n",
    "            kmed = INDEX[(INDEX.k>0.3) & (INDEX.k<=0.6)]\n",
    "            khigh =  INDEX[(INDEX.k>0.6)]\n",
    "            klow_test = klow.sample(frac=test_size,random_state=random_state)\n",
    "            kmed_test = kmed.sample(frac=test_size,random_state=random_state)\n",
    "            khigh_test = khigh.sample(frac=test_size,random_state=random_state)\n",
    "            X_test = pd.concat([klow_test, kmed_test, khigh_test])\n",
    "\n",
    "        else:\n",
    "            X_test = INDEX.sample(frac=test_size, random_state = random_state)\n",
    "            \n",
    "        X_test.reset_index(inplace=True)\n",
    "        X_test = X_test.rename(columns={'index':'date'})\n",
    "        X_test = X_test.drop(columns = ['k'])\n",
    "        X_test = df.reset_index().merge(X_test,on=['date','site']).set_index('datetime')\n",
    "        X_train = pd.concat([df,X_test]).drop_duplicates(keep=False)\n",
    "        X_train.drop(columns=['date'],inplace = True); X_test.drop(columns=['date'],inplace = True)\n",
    "        \n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1275426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "train_data, test_data = train_test_splits(df, test_size=0.2, random_state=42, split_by_k = True)\n",
    "train_ATT, test_ATT = train_test_splits(ATT, test_size=0.2, random_state=42, split_by_k = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533cd9e",
   "metadata": {},
   "source": [
    "# Cloud2K Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36927a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud2K_name = ['Cloud2Kv1','Cloud2Kv2','Cloud2Kv3','Cloud2Kv4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81d7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resblock(x, kernelsize, filters):\n",
    "    fx = layers.Conv2D(filters, kernelsize, padding='same')(x)\n",
    "    fx = layers.BatchNormalization()(fx)\n",
    "    act1 = layers.Activation(activation='relu')(fx)\n",
    "    fx = layers.Conv2D(filters, kernelsize, padding='same')(act1)\n",
    "    out = layers.Add()([x,fx])\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.ReLU()(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud2K_list = []\n",
    "# Define the CNN architecture\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "conv1 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(img_inputs)\n",
    "conv2 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv1)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv2)\n",
    "flatten_cnn = layers.Flatten()(pool1)\n",
    "dense1 = layers.Dense(16, activation='relu')(flatten_cnn)\n",
    "dense2 = layers.Dense(8, activation='relu')(dense1)\n",
    "outputs = layers.Dense(1)(dense2)\n",
    "Cloud2Kv1 = keras.Model(inputs=img_inputs, outputs=outputs, name=\"Cloud2Kv1\")\n",
    "Cloud2K_list.append(Cloud2Kv1)\n",
    "Cloud2Kv1.summary()\n",
    "\n",
    "# Define the CNN architecture\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "conv1 = layers.Conv2D(32, kernel_size=(5, 5), activation='relu',padding='same')(img_inputs)\n",
    "conv2 = layers.Conv2D(32, kernel_size=(5, 5), activation='relu',padding='same')(conv1)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv2)\n",
    "flatten_cnn = layers.Flatten()(pool1)\n",
    "dense1 = layers.Dense(16, activation='relu')(flatten_cnn)\n",
    "dense3 = layers.Dense(8, activation='relu')(dense1)\n",
    "outputs = layers.Dense(1)(dense3)\n",
    "Cloud2Kv2 = keras.Model(inputs=img_inputs, outputs=outputs, name=\"Cloud2Kv2\")\n",
    "Cloud2K_list.append(Cloud2Kv2)\n",
    "Cloud2Kv2.summary()\n",
    "\n",
    "# Define the CNN architecture\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "conv1 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(img_inputs)\n",
    "conv2 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv1)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv2)\n",
    "conv3 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(pool1)\n",
    "conv4 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv3)\n",
    "pool2 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv4)\n",
    "conv5 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(pool2)\n",
    "conv6 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv5)\n",
    "pool3 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv6)\n",
    "flatten_cnn = layers.Flatten()(pool3)\n",
    "dense1 = layers.Dense(64, activation='relu')(flatten_cnn)\n",
    "dense2 = layers.Dense(16, activation='relu')(dense1)\n",
    "outputs = layers.Dense(1)(dense2)\n",
    "Cloud2Kv3 = keras.Model(inputs=img_inputs, outputs=outputs, name=\"Cloud2Kv3\")\n",
    "Cloud2K_list.append(Cloud2Kv3)\n",
    "Cloud2Kv3.summary()\n",
    "\n",
    "# Define the CNN architecture\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "conv1 = layers.Conv2D(8, (5, 5), padding='same')(img_inputs)\n",
    "batch1 = layers.BatchNormalization()(conv1)\n",
    "act1 = layers.Activation(activation='relu')(batch1)\n",
    "conv2 = layers.Conv2D(8, (5, 5), padding='same')(act1)\n",
    "batch2 = layers.BatchNormalization()(conv2)\n",
    "act2 = layers.Activation(activation='relu')(batch2)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(act2)\n",
    "conv3 = layers.Conv2D(16, (1, 1), padding='same')(pool1)\n",
    "batch3 = layers.BatchNormalization()(conv3)\n",
    "act3 = layers.Activation(activation='relu')(batch3)\n",
    "resbox1 = resblock(batch3, (5, 5), 16)\n",
    "resbox2 = resblock(resbox1, (5, 5), 16)\n",
    "resbox3 = resblock(resbox2, (5, 5), 16)\n",
    "pool3 = layers.MaxPooling2D((2, 2),strides=(2,2))(resbox3)\n",
    "conv4 = layers.Conv2D(32, (1, 1), padding='same')(pool3)\n",
    "batch4 = layers.BatchNormalization()(conv4)\n",
    "act4 = layers.Activation(activation='relu')(batch4)\n",
    "resbox4 = resblock(act4, (5, 5), 32)\n",
    "resbox5 = resblock(resbox4, (5, 5), 32)\n",
    "resbox6 = resblock(resbox5, (5, 5), 32)\n",
    "pool4 = layers.AveragePooling2D((4, 4))(resbox6)\n",
    "flatten_cnn = layers.Flatten()(pool4)\n",
    "dense1 = layers.Dense(32,activation='relu')(flatten_cnn)\n",
    "dense2 = layers.Dense(8,activation='relu')(dense1)\n",
    "outputs = layers.Dense(1)(dense2)\n",
    "Cloud2Kv4 = keras.Model(inputs=img_inputs, outputs=outputs, name=\"Cloud2Kv4\")\n",
    "Cloud2K_list.append(Cloud2Kv4)\n",
    "Cloud2Kv4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da42644",
   "metadata": {},
   "source": [
    "# Cloud2I Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud2I_name = ['Cloud2Iv1','Cloud2Iv2','Cloud2Iv3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resblock(x, kernelsize, filters):\n",
    "    fx = layers.Conv2D(filters, kernelsize, padding='same')(x)\n",
    "    act1 = layers.Activation(activation='relu')(fx)\n",
    "    fx = layers.Conv2D(filters, kernelsize, padding='same')(act1)\n",
    "    out = layers.Add()([x,fx])\n",
    "    out = layers.ReLU()(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e076a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cloud2I_list = []\n",
    "# define CNN model\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "ATT_input = keras.Input(shape=(4))\n",
    "conv1 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(img_inputs)\n",
    "conv2 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv1)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv2)\n",
    "flatten_cnn = layers.Flatten()(pool1)\n",
    "dense1 = layers.Dense(4)(ATT_input) # define MLP model\n",
    "flatten_mlp = layers.Flatten()(dense1)\n",
    "merged = layers.concatenate([flatten_cnn, flatten_mlp]) # combine CNN and MLP\n",
    "dense3 = layers.Dense(256, activation='relu')(merged) \n",
    "dense4 = layers.Dense(32, activation='relu')(dense3)\n",
    "outputs = layers.Dense(1)(dense4)\n",
    "Cloud2Iv1 = keras.Model(inputs=[img_inputs, ATT_input], outputs=outputs, name=\"Cloud2Iv1\")\n",
    "Cloud2I_list.append(Cloud2Iv1)\n",
    "Cloud2Iv1.summary()\n",
    "\n",
    "# define CNN model\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "ATT_input = keras.Input(shape=(4))\n",
    "conv1 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(img_inputs)\n",
    "conv2 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv1)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv2)\n",
    "conv3 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(pool1)\n",
    "conv4 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv3)\n",
    "pool2 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv4)\n",
    "conv5 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(pool2)\n",
    "conv6 = layers.Conv2D(16, kernel_size=(5, 5), activation='relu',padding='same')(conv5)\n",
    "pool3 = layers.MaxPooling2D((2, 2),strides=(2,2))(conv6)\n",
    "flatten_cnn = layers.Flatten()(pool3)\n",
    "dense1 = layers.Dense(4)(ATT_input) # define MLP model\n",
    "flatten_mlp = layers.Flatten()(dense1)\n",
    "merged = layers.concatenate([flatten_cnn, flatten_mlp]) # combine CNN and MLP\n",
    "dense3 = layers.Dense(64, activation='relu')(merged)\n",
    "dense4 = layers.Dense(16, activation='relu')(dense3)\n",
    "outputs = layers.Dense(1)(dense4)\n",
    "Cloud2Iv2 = keras.Model(inputs=[img_inputs, ATT_input], outputs=outputs, name=\"Cloud2Iv2\")\n",
    "Cloud2I_list.append(Cloud2Iv2)\n",
    "Cloud2Iv2.summary()\n",
    "\n",
    "\n",
    "# define CNN model\n",
    "input_shape = (16, 16, 1)\n",
    "img_inputs = keras.Input(shape=input_shape)\n",
    "ATT_input = keras.Input(shape=(4))\n",
    "conv1 = layers.Conv2D(8, (5, 5), padding='same')(img_inputs)\n",
    "batch1 = layers.BatchNormalization()(conv1)\n",
    "act1 = layers.Activation(activation='relu')(batch1)\n",
    "conv2 = layers.Conv2D(8, (5, 5), padding='same')(act1)\n",
    "batch2 = layers.BatchNormalization()(conv2)\n",
    "act2 = layers.Activation(activation='relu')(batch2)\n",
    "pool1 = layers.MaxPooling2D((2, 2),strides=(2,2))(act2)\n",
    "conv3 = layers.Conv2D(16, (1, 1), padding='same')(pool1)\n",
    "batch3 = layers.BatchNormalization()(conv3)\n",
    "act3 = layers.Activation(activation='relu')(batch3)\n",
    "resbox1 = resblock(batch3, (5, 5), 16)\n",
    "resbox2 = resblock(resbox1, (5, 5), 16)\n",
    "resbox3 = resblock(resbox2, (5, 5), 16)\n",
    "pool3 = layers.MaxPooling2D((2, 2),strides=(2,2))(resbox3)\n",
    "conv4 = layers.Conv2D(32, (1, 1), padding='same')(pool3)\n",
    "batch4 = layers.BatchNormalization()(conv4)\n",
    "resbox4 = resblock(batch4, (5, 5), 32)\n",
    "resbox5 = resblock(resbox4, (5, 5), 32)\n",
    "resbox6 = resblock(resbox5, (5, 5), 32)\n",
    "pool4 = layers.AveragePooling2D((4, 4))(resbox6)\n",
    "flatten_cnn = layers.Flatten()(pool4)\n",
    "dense1 = layers.Dense(4)(ATT_input) # define MLP model\n",
    "flatten_mlp = layers.Flatten()(dense1)\n",
    "merged = layers.concatenate([flatten_cnn, flatten_mlp])\n",
    "dense2 = layers.Dense(32,activation='relu')(merged)\n",
    "dense3 = layers.Dense(8,activation='relu')(dense2)\n",
    "outputs = layers.Dense(1)(dense3)\n",
    "Cloud2Iv3 = keras.Model(inputs=[img_inputs, ATT_input], outputs=outputs, name=\"Cloud2Iv3\")\n",
    "Cloud2I_list.append(Cloud2Iv3)\n",
    "Cloud2Iv3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9945fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Cloud2K model\n",
    "TargetVariable=['k']\n",
    "Predictors=list(df.columns.drop(['site','k','Iclr','CI0','I','date']))\n",
    " \n",
    "X_train=train_data[Predictors].values.reshape(-1,16,16,1)\n",
    "y_train=train_data[TargetVariable].values\n",
    "X_test = test_data[Predictors].values.reshape(-1,16,16,1)\n",
    "y_test =test_data[TargetVariable].values\n",
    "\n",
    "X_train=X_train/255\n",
    "X_test=X_test/255\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split train and validate set \n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f972f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and train the Cloud2K model\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "batch_size = 32\n",
    "epochs=500\n",
    "\n",
    "\n",
    "for index, model in enumerate(Cloud2K_list) :\n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 50, \n",
    "                                        restore_best_weights = True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.001)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.002)\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = 'mean_absolute_error')\n",
    "    Cloud2K_name[index] = model.fit(X_train_train, y_train_train, epochs = epochs, \n",
    "                        batch_size = batch_size,validation_data = (X_valid, y_valid) ,\n",
    "                        verbose = 1,callbacks = [earlystopping, reduce_lr] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0398556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save Cloud2K model\n",
    "\n",
    "name = ['Cloud2K_1','Cloud2K_2','Cloud2K_3','Cloud2K_4']\n",
    "for i,save_model in enumerate(Cloud2K_list):\n",
    "    save_model.save(str(name[i]+'.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2978dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save loss and val_loss in each epoch to .CSV\n",
    "\n",
    "hist_csv_file = ['hisc2kv1.csv','hisc2kv2.csv','hisc2kv3.csv','hisc2kv4.csv']\n",
    "for i,model in enumerate(Cloud2K_name):\n",
    "    loss = model.history['loss']\n",
    "    val_loss = model.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    valid_loss = pd.DataFrame(dict(epochs = range(len(loss)),loss = model.history['loss'],val_loss = model.history['val_loss']))\n",
    "    \n",
    "    with open(hist_csv_file[i], mode='w') as f:\n",
    "        valid_loss.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Cloud2I model\n",
    "\n",
    "TargetVariable = ['I']\n",
    "Predictors = list(df.columns.drop(['site','k','Iclr','CI0','I','date']))\n",
    "\n",
    "X_train = train_data[Predictors].values.reshape(-1,16,16,1)\n",
    "y_train = train_data[TargetVariable].values\n",
    "X_test = test_data[Predictors].values.reshape(-1,16,16,1)\n",
    "y_test = test_data[TargetVariable].values\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "ATT_taget = ['I']\n",
    "ATT_predictors = list(ATT.columns.drop(['I','site','k']))\n",
    "X_train_ATT = train_ATT[ATT_predictors].values\n",
    "y_train_ATT = train_ATT[ATT_taget].values\n",
    "X_test_ATT = test_ATT[ATT_predictors].values\n",
    "y_test_ATT = test_ATT[ATT_taget].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#split train and validate set \n",
    "X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, shuffle = False) \n",
    "X_train_train_ATT, X_val_ATT, y_train_train_ATT, y_val_ATT = train_test_split(X_train_ATT, y_train_AT, test_size=0.25, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01998315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile and train the Cloud2I model\n",
    "\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "batch_size = 32\n",
    "epochs=500\n",
    "\n",
    "for index,model in enumerate(Cloud2I_list) :\n",
    "    earlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n",
    "                                        mode =\"min\", patience = 10, \n",
    "                                        restore_best_weights = True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.001)\n",
    "    optimizer = keras.optimizers.Adam(lr=0.002)\n",
    "    model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "    Cloud2I_name[index]=model.fit([X_train_train, X_train_train_ATT], y_train_train, epochs=epochs, \n",
    "                              batch_size = batch_size, validation_data=([X_valid, X_val_ATT], y_valid) ,\n",
    "                              verbose=1,callbacks =[earlystopping,reduce_lr] ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31649c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save Cloud2I model\n",
    "name = ['Cloud2I_1','Cloud2I_2','Cloud2I_3']\n",
    "for i,save_model in enumerate(Cloud2I_list):\n",
    "    save_model.save(str(name[i]+'.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04891dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#save loss and val_loss in each epoch to .CSV\n",
    "\n",
    "hist_csv_file = ['hisc2I_1.csv','hisc2I_2.csv','hisc2I_3.csv']\n",
    "for i,model in enumerate(Cloud2I_name):\n",
    "    loss = model.history['loss']\n",
    "    val_loss = model.history['val_loss']\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    valid_loss = pd.DataFrame(dict(epochs = range(len(loss)),loss = model.history['loss'],val_loss = model.history['val_loss']))\n",
    "    \n",
    "    with open(hist_csv_file[i], mode='w') as f:\n",
    "        valid_loss.to_csv(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29db9c0c",
   "metadata": {},
   "source": [
    "# Model testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.listdir('/Users/khunnoot/Desktop/year4/senior_project/jupyter_notebook')\n",
    "\n",
    "file_Cloud2K = ['Cloud2K_1.h5','Cloud2K_2.h5','Cloud2K_3.h5','Cloud2K_4.h5']\n",
    "file_Cloud2I = ['Cloud2I_1.h5','Cloud2I_2.h5','Cloud2I_3.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def model(file): #load model and keep in the list \n",
    "    model = []\n",
    "    for name in file:\n",
    "        model.append(load_model(os.path.join(Directory,name)))\n",
    "    return model\n",
    "def get_name(file) : # get name of each model and add in the list\n",
    "    name = []\n",
    "    for i in file :\n",
    "        i = i[:-3]\n",
    "        name.append(i)\n",
    "    return name \n",
    "def k_to_I(df,k_columns,I_columns): #change estimated k to estimated I\n",
    "    col_name = []\n",
    "    for i,c in enumerate(k_columns):\n",
    "        name = 'I_'+c\n",
    "        df=df.assign( col = lambda x: (x[c] * x['Iclr']))\n",
    "        df.rename(columns={'col':name},inplace=True)\n",
    "        col_name.append(name)\n",
    "    for i in I_columns :\n",
    "        name = 'I_' + i\n",
    "        col_name.append(name)\n",
    "    return df[['site','I']+col_name]\n",
    "def AE(df,I_columns): #generate absolute error\n",
    "    name = []\n",
    "    for i,c in enumerate(I_columns):\n",
    "        AE_name = 'AE_'+c[2:]\n",
    "        df=df.assign( col = lambda x: abs(x[c] - x['I']))\n",
    "        df.rename(columns={'col':AE_name},inplace=True)\n",
    "        SE_name = 'SE_'+c[2:]\n",
    "        df=df.assign( col = lambda x: (x[c] - x['I'])**2)\n",
    "        df.rename(columns={'col':SE_name},inplace=True)\n",
    "        name.append(AE_name)\n",
    "    return df[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea322d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Cloud2K = model(file_Cloud2K)\n",
    "model_Cloud2I = model(file_Cloud2I)\n",
    "Cloud2K = get_name(file_Cloud2K)\n",
    "Cloud2I =  get_name(file_Cloud2I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9990b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_model generate the model prediction when the input is 'df,ATT' = dataframe, Cloud2Kname and Cloud2Iname is \n",
    "# is list of name of each models , model_Cloud2K and model_Cloud2I is list of each models \n",
    "# if you do not have any model, you can insert only '[]'\n",
    "\n",
    "def pred_model(df, ATT, Cloud2Kname, Cloud2Iname, model_Cloud2K, model_Cloud2I):\n",
    "    #train test split\n",
    "    train_data, test_data = train_test_splits(df, test_size=0.2, random_state=42, split_by_k = True)\n",
    "    train_ATT, test_ATT = train_test_splits(ATT, test_size=0.2, random_state=42, split_by_k = True)\n",
    "\n",
    "    #for map k\n",
    "    TargetVariable=['k']\n",
    "    Predictors=list(df.columns.drop(['site','k','Iclr','CI0','I','date']))\n",
    "    X_train=train_data[Predictors].values.reshape(-1,16,16,1)\n",
    "    y_train=train_data[TargetVariable].values\n",
    "    X_test = test_data[Predictors].values.reshape(-1,16,16,1)\n",
    "    y_test =test_data[TargetVariable].values\n",
    "    X_train=X_train/255\n",
    "    X_test=X_test/255\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #split train and validate set \n",
    "    X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, shuffle = False) \n",
    "\n",
    "    ## run train test split of model map k\n",
    "    result=test_data[['site','Iclr','I']+TargetVariable].copy()\n",
    "    for i,model in enumerate(model_Cloud2K):\n",
    "        k_pred=model.predict(X_test)\n",
    "        result['k_predict']=k_pred\n",
    "        result=result.rename(columns={'k_predict':Cloud2Kname[i]})\n",
    "\n",
    "    #for map I\n",
    "\n",
    "    TargetVariable=['I']\n",
    "    Predictors=list(df.columns.drop(['site','k','Iclr','CI0','I','date']))\n",
    "    X_train=train_data[Predictors].values.reshape(-1,16,16,1)\n",
    "    y_train=train_data[TargetVariable].values\n",
    "    X_test = test_data[Predictors].values.reshape(-1,16,16,1)\n",
    "    y_test =test_data[TargetVariable].values\n",
    "    X_train=X_train/255\n",
    "    X_test=X_test/255\n",
    "\n",
    "    ATT_taget = ['I']\n",
    "    ATT_predictors = list(ATT.columns.drop(['I','site','k','date']))\n",
    "    X_train_ATT = train_ATT[ATT_predictors].values\n",
    "    y_train_ATT = train_ATT[ATT_taget].values\n",
    "    X_test_ATT = test_ATT[ATT_predictors].values\n",
    "    y_test_ATT =test_ATT[ATT_taget].values\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #split train and validate set \n",
    "    X_train_train, X_valid, y_train_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, shuffle = False) \n",
    "    X_train_train_ATT, X_val_ATT, y_train_train_ATT, y_val_ATT = train_test_split(X_train_ATT, y_train_ATT, test_size=0.25, shuffle = False)\n",
    "\n",
    "    ## run train test split of model map I\n",
    "    for i,model in enumerate(model_Cloud2I):\n",
    "        I_pred=model.predict([X_test,X_test_ATT])\n",
    "        result['I_predict']=I_pred\n",
    "        result=result.rename(columns={'I_predict':'I_'+Cloud2Iname[i]})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40297bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = pred_model(df,ATT,Cloud2K, Cloud2I, model_Cloud2K, model_Cloud2I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ihat = k_to_I(output, Cloud2K, Cloud2I)\n",
    "Ihat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e740e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ihat.to_csv('Ihat_1layer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAE in each model\n",
    "I_col = Ihat.drop(columns=['site','I'])\n",
    "err_AE = AE(Ihat,I_col)\n",
    "err_AE.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
